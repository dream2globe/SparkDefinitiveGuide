{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 스파크 SQL\n",
    "+ 스파크 SQL은 하이브 메타스토어를 사용하므로 하이브와 잘 연동됨\n",
    "    + 하이브 버전 설정: spark.sql.hive.metastore.version\n",
    "    + 초기화 방식 변경: spark.sql.hive.metastore.jars\n",
    "    + 클래스 접두사 설정: spark.sql.hive.metastore.sharedPrefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 스파크 쿼리 실행방법\n",
    "+ 스파크 SQL CLI\n",
    "    ```\n",
    "    ./bin/spark-sql\n",
    "    ```\n",
    "+ 스파크 프로그래밍 SQL 인터페이스\n",
    "    ```\n",
    "    spark.sql(\"SELECT 1+1\").show()\n",
    "    ```\n",
    "+ 스파크 SQL 쓰리프트 JDBC/ODBC 서버\n",
    "    + 자바 데이터베이스 연결 인터페이스 제공\n",
    "    + 하이브 1.2.1. 버전의 HiveServer2에 맞추어 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|   DEST_COUNTRY_NAME|sum(count)|\n",
      "+--------------------+----------+\n",
      "|            Anguilla|        41|\n",
      "|              Russia|       176|\n",
      "|            Paraguay|        60|\n",
      "|             Senegal|        40|\n",
      "|              Sweden|       118|\n",
      "|            Kiribati|        26|\n",
      "|              Guyana|        64|\n",
      "|         Philippines|       134|\n",
      "|            Djibouti|         1|\n",
      "|            Malaysia|         2|\n",
      "|           Singapore|         3|\n",
      "|                Fiji|        24|\n",
      "|              Turkey|       138|\n",
      "|                Iraq|         1|\n",
      "|             Germany|      1468|\n",
      "|              Jordan|        44|\n",
      "|               Palau|        30|\n",
      "|Turks and Caicos ...|       230|\n",
      "|              France|       935|\n",
      "|              Greece|        30|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 프로그래밍 SQL 인터페이스\n",
    "spark.read.json(\"./data/flight-data/json/2015-summary.json\").createOrReplaceTempView(\"flight\")\n",
    "\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM flight GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 카탈로그\n",
    "+ 스파크의 가장 높은 추상화 단계\n",
    "+ 테이블, 데이터베이스, 함수를 조회하는 등 여러 가지 유용한 함수를 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 테이블\n",
    "+ DataFrame과 논리적으로 동일\n",
    "+ 스파크에서 테이블을 생성하면 default 데이터베이스에 등록됨\n",
    "+ 임시 테이블 개념이 없으며 데이터를 가지지 않은 뷰만 존재\n",
    "+ 관리형 테이블과 외부 테이블이 있으며 디스크에 저장된 파일을 이용해 테이블을 정의하면 외부 테이블을 활용하는 것임\n",
    "\n",
    "### 10.6.2 테이블 생성하기\n",
    "+ USING을 통해 포맷을 지정하지 않으면 스파크는 기본적으로 하이브 SerDe 설정을 사용해서 Reader, Writer 성능에 악영향을 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS flights(\n",
    "    DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "USING JSON OPTIONS (path './data/flight-data/json/2015-summary.json')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 해당 폴더를 지우는 명령어\n",
    "! rm -r -f ./spark-warehouse/flights_from_select\n",
    "\n",
    "# 특정 테이블에서 원하는 데이터만 추출해서 새로운 테이블을 생성(CTAS 패턴)\n",
    "spark.sql( \n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS flights_from_select\n",
    "USING Parquet AS SELECT * FROM flights\n",
    "\"\"\") # spark-warehouse 폴더에 테이블 이름의 서브폴더가 생성되고 그 안에 파케이 파일이 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6.3 외부 테이블 생성하기\n",
    "+ 외부 테이블의 메타데이터를 관리하지만 데이터 파일은 스파크에서 관리하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS\n",
    "    hive_flights(DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "LOCATION './data/flight-data-hive'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "CREATE EXTERNAL TABLE hive_flights2\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "LOCATION './data/flight-data-hive' AS SELECT * FROM flights\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6.4 테이블에 데이터 삽입하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DEST_COUNTRY_NAME)|\n",
      "+------------------------+\n",
      "|                     380|\n",
      "+------------------------+\n",
      "\n",
      "+------------------------+\n",
      "|count(DEST_COUNTRY_NAME)|\n",
      "+------------------------+\n",
      "|                     400|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT count(DEST_COUNTRY_NAME) FROM flights_from_select\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "INSERT INTO flights_from_select\n",
    "    SELECT * FROM flights LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT count(DEST_COUNTRY_NAME) FROM flights_from_select\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|  DEST_COUNTRY_NAME|   string|   null|\n",
      "|ORIGIN_COUNTRY_NAME|   string|   null|\n",
      "|              count|   bigint|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql( # 메타 데이터 확인\n",
    "\"\"\"\n",
    "DESCRIBE TABLE flights\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기타 명령어\n",
    "+ DESCRIBE: 메타정보 확인\n",
    "+ SHOW PARTITIONS: 파티셔닝 스키마 정보 확인\n",
    "+ REFRESH: 캐싱된 항목을 갱신\n",
    "+ MSCK REPAIR TABLE: 관리하는 테이블의 파티션 정보를 새로 고침\n",
    "+ DROP TABLE: 외부 테이블을 제거하면 데이터는 삭제되지 않지마, 외부 테이블명을 이용해 데이터 조회는 안됨\n",
    "+ CACHE/UNCACHE TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.10 고급 주제\n",
    "+ 구조체, 리스트, 맵 세가지 타입이 존재\n",
    "\n",
    "#### 리스트\n",
    "+ 값의 리스트를 만드는 collect_list, 중복값을 제거하는 collect_set이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---------------+\n",
      "|            new_name|flight_counts|     origin_set|\n",
      "+--------------------+-------------+---------------+\n",
      "|            Anguilla|         [41]|[United States]|\n",
      "|            Paraguay|         [60]|[United States]|\n",
      "|              Russia|        [176]|[United States]|\n",
      "|             Senegal|         [40]|[United States]|\n",
      "|              Sweden|        [118]|[United States]|\n",
      "|            Kiribati|         [26]|[United States]|\n",
      "|              Guyana|         [64]|[United States]|\n",
      "|         Philippines|        [134]|[United States]|\n",
      "|            Djibouti|          [1]|[United States]|\n",
      "|            Malaysia|          [2]|[United States]|\n",
      "|           Singapore|          [3]|[United States]|\n",
      "|                Fiji|         [24]|[United States]|\n",
      "|              Turkey|        [138]|[United States]|\n",
      "|                Iraq|          [1]|[United States]|\n",
      "|             Germany|       [1468]|[United States]|\n",
      "|              Jordan|         [44]|[United States]|\n",
      "|               Palau|         [30]|[United States]|\n",
      "|              France|        [935]|[United States]|\n",
      "|Turks and Caicos ...|        [230]|[United States]|\n",
      "|              Greece|         [30]|[United States]|\n",
      "+--------------------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT DEST_COUNTRY_NAME as new_name, collect_list(count) as flight_counts,\n",
    "    collect_set(ORIGIN_COUNTRY_NAME) as origin_set\n",
    "FROM flights GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|   DEST_COUNTRY_NAME|array(1, 2, 3)|\n",
      "+--------------------+--------------+\n",
      "|       United States|     [1, 2, 3]|\n",
      "|       United States|     [1, 2, 3]|\n",
      "|       United States|     [1, 2, 3]|\n",
      "|               Egypt|     [1, 2, 3]|\n",
      "|       United States|     [1, 2, 3]|\n",
      "|       United States|     [1, 2, 3]|\n",
      "|       United States|     [1, 2, 3]|\n",
      "|          Costa Rica|     [1, 2, 3]|\n",
      "|             Senegal|     [1, 2, 3]|\n",
      "|             Moldova|     [1, 2, 3]|\n",
      "|       United States|     [1, 2, 3]|\n",
      "|       United States|     [1, 2, 3]|\n",
      "|              Guyana|     [1, 2, 3]|\n",
      "|               Malta|     [1, 2, 3]|\n",
      "|            Anguilla|     [1, 2, 3]|\n",
      "|             Bolivia|     [1, 2, 3]|\n",
      "|       United States|     [1, 2, 3]|\n",
      "|             Algeria|     [1, 2, 3]|\n",
      "|Turks and Caicos ...|     [1, 2, 3]|\n",
      "|       United States|     [1, 2, 3]|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, ARRAY(1, 2, 3) FROM flights\n",
    "\"\"\" # 직접 베열 입력\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|   DEST_COUNTRY_NAME|array(1, 2, 3)[0]|\n",
      "+--------------------+-----------------+\n",
      "|       United States|                1|\n",
      "|       United States|                1|\n",
      "|       United States|                1|\n",
      "|               Egypt|                1|\n",
      "|       United States|                1|\n",
      "|       United States|                1|\n",
      "|       United States|                1|\n",
      "|          Costa Rica|                1|\n",
      "|             Senegal|                1|\n",
      "|             Moldova|                1|\n",
      "|       United States|                1|\n",
      "|       United States|                1|\n",
      "|              Guyana|                1|\n",
      "|               Malta|                1|\n",
      "|            Anguilla|                1|\n",
      "|             Bolivia|                1|\n",
      "|       United States|                1|\n",
      "|             Algeria|                1|\n",
      "|Turks and Caicos ...|                1|\n",
      "|       United States|                1|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, ARRAY(1, 2, 3)[0] FROM flights \n",
    "\"\"\" # 위치 인덱싱\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect_list로 count 값들을 묶음 \n",
    "sql = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW flights_agg AS\n",
    "    SELECT DEST_COUNTRY_NAME, collect_list(count) as flight_counts\n",
    "    FROM flights GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|count|   DEST_COUNTRY_NAME|\n",
      "+-----+--------------------+\n",
      "|   41|            Anguilla|\n",
      "|   60|            Paraguay|\n",
      "|  176|              Russia|\n",
      "|   40|             Senegal|\n",
      "|  118|              Sweden|\n",
      "|   26|            Kiribati|\n",
      "|   64|              Guyana|\n",
      "|  134|         Philippines|\n",
      "|    1|            Djibouti|\n",
      "|    2|            Malaysia|\n",
      "|    3|           Singapore|\n",
      "|   24|                Fiji|\n",
      "|  138|              Turkey|\n",
      "|    1|                Iraq|\n",
      "| 1468|             Germany|\n",
      "|   44|              Jordan|\n",
      "|   30|               Palau|\n",
      "|  935|              France|\n",
      "|  230|Turks and Caicos ...|\n",
      "|   30|              Greece|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 반대로 동작\n",
    "sql = \"\"\"\n",
    "SELECT explode(flight_counts) as count, DEST_COUNTRY_NAME FROM flights_agg\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.10.2 함수\n",
    "+ SHOW FUNCTIONS\n",
    "+ SHOW SYSTEM FUNCTIONS\n",
    "+ SHOW USER FUNCTIONS\n",
    "+ SHOW FUNCTIONS \"S*\"\n",
    "+ SHOW FUNCTIONS LIKE \"collect*\"\n",
    "+ 사용자 정의 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|count|power3(count)|\n",
      "+-----+-------------+\n",
      "|   15|         3375|\n",
      "|    1|            1|\n",
      "|  344|     40707584|\n",
      "|   15|         3375|\n",
      "|   62|       238328|\n",
      "|    1|            1|\n",
      "|   62|       238328|\n",
      "|  588|    203297472|\n",
      "|   40|        64000|\n",
      "|    1|            1|\n",
      "|  325|     34328125|\n",
      "|   39|        59319|\n",
      "|   64|       262144|\n",
      "|    1|            1|\n",
      "|   41|        68921|\n",
      "|   30|        27000|\n",
      "|    6|          216|\n",
      "|    4|           64|\n",
      "|  230|     12167000|\n",
      "|    1|            1|\n",
      "+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def power3(num):\n",
    "    return num * num * num\n",
    "\n",
    "spark.udf.register(\"power3\", f=power3)\n",
    "\n",
    "sql = \"SELECT count, power3(count) FROM flight\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.10.3 서브 쿼리\n",
    "+ 비상호 연관쿼리: 서브쿼리와 연관된 정보를 사용하지 않음\n",
    "+ 상호 연관쿼리: 내부쿼리가 외부 쿼리의 결과를 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비상호 연관쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|dest_country_name|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|           Canada|\n",
      "|           Mexico|\n",
      "|   United Kingdom|\n",
      "|            Japan|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subsql = \"\"\"\n",
    "SELECT dest_country_name FROM flights\n",
    "GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5\n",
    "\"\"\"\n",
    "spark.sql(subsql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Egypt|      United States|   15|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  382|\n",
      "|            Pakistan|      United States|   12|\n",
      "|             Iceland|      United States|  181|\n",
      "|    Marshall Islands|      United States|   42|\n",
      "|          Luxembourg|      United States|  155|\n",
      "|            Honduras|      United States|  362|\n",
      "|         The Bahamas|      United States|  955|\n",
      "|         El Salvador|      United States|  561|\n",
      "|               Samoa|      United States|   25|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT * FROM flights\n",
    "WHERE origin_country_name IN\n",
    "    (SELECT dest_country_name FROM flights\n",
    "    GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5)\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 상호 연관쿼리\n",
    "목적기 국가에서 되돌아올 수 있는 항공편이 있는지 알고 싶다면 목적지 국가를 출발지 국가로, 출발지 국가를 목적기 국가로 설정하여 항공편이 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|       United States|             Romania|   15|\n",
      "|       United States|             Croatia|    1|\n",
      "|       United States|             Ireland|  344|\n",
      "|               Egypt|       United States|   15|\n",
      "|       United States|               India|   62|\n",
      "|       United States|           Singapore|    1|\n",
      "|       United States|             Grenada|   62|\n",
      "|          Costa Rica|       United States|  588|\n",
      "|             Senegal|       United States|   40|\n",
      "|       United States|        Sint Maarten|  325|\n",
      "|       United States|    Marshall Islands|   39|\n",
      "|              Guyana|       United States|   64|\n",
      "|               Malta|       United States|    1|\n",
      "|            Anguilla|       United States|   41|\n",
      "|             Bolivia|       United States|   30|\n",
      "|       United States|            Paraguay|    6|\n",
      "|Turks and Caicos ...|       United States|  230|\n",
      "|               Italy|       United States|  382|\n",
      "|       United States|Federated States ...|   69|\n",
      "|       United States|              Russia|  161|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT * FROM flights f1\n",
    "WHERE EXISTS (SELECT 1 FROM flights f2\n",
    "    WHERE f1.dest_country_name = f2.origin_country_name)\n",
    "AND EXISTS (SELECT 1 FROM flights f2\n",
    "    WHERE f2.dest_country_name = f1.origin_country_name)\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
