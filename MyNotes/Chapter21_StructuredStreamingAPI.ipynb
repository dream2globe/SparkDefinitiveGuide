{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 21 구조적 스트리밍의 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.2 핵심 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2.1 트랜스포메이션과 액션\n",
    "+ 트렌스포메이션과 유사하나 증분 처리를 할 수 없는 일부 쿼리 유형은 사용 제약이 있을 수 있음\n",
    "+ 구조적 스트리밍에는 스트림 처리를 시작한 뒤 연속적으로 처리해 결과를 출력하는 한 가지 액션만 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2.2 입력 소스\n",
    "+ 스트리밍 방식으로 데이터를 읽을 수 있는 소스\n",
    "    + 아파치 카프카 0.10 버전\n",
    "    + HDFS나 S3 등 분산 파일시스템의 파일(스파크는 디렉터리의 신규 파일을 계속해서 읽음)\n",
    "    + 테스트용 소켓 소스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2.3 싱크\n",
    "+ 스트림의 결과를 저장할 목적지\n",
    "    + 아파치 카프카 0.10\n",
    "    + 거의 모든 파일 포맷\n",
    "    + 출력 레코드에 임의 연산을 실행하는 foreach 싱크\n",
    "    + 테스트용 콘솔 싱크\n",
    "    + 디버깅용 콘솔 싱크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2.4 출력 모드\n",
    "+ 데이터를 출력하는 방법의 정의\n",
    "    + append: 싱크에 신규 레코드만 추가\n",
    "    + update: 변경 대상 레코드 자체를 갱신\n",
    "    + complete: 전체 출력 내용 재작성 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2.5 트리거\n",
    "+ 데이터 출력 시점을 정의\n",
    "+ 기본적으로 마지막 입력 데이터를 처리한 직후에 신규 입력 데이터를 조회해 최단 시간 내에 새로운 처리 결과를 만들어 냄\n",
    "+ 작은 크기의 파일이 여러 개 생실 수 있기 때문에 처리 시간 기반의 트리거도 지원함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2.6 이벤트 시간 처리\n",
    "+ 데이터에 기록되 시간 필드 기준으로 데이터를 처리함을 의미\n",
    "+ 워터마크: 시간 제한을 설정할 수 있는 스트리밍 시스템의 기능으로 늦게 들어온 이벤트를 어디까지 처리할지 시간을 제한할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.3 구조적 스트리밍 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정적인 방식의 데이터셋 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션 생성\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark structured streaming example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = spark.read.json(\"../data/activity-data/\")\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Arrival_Time,LongType,true),StructField(Creation_Time,LongType,true),StructField(Device,StringType,true),StructField(Index,LongType,true),StructField(Model,StringType,true),StructField(User,StringType,true),StructField(gt,StringType,true),StructField(x,DoubleType,true),StructField(y,DoubleType,true),StructField(z,DoubleType,true)))\n"
     ]
    }
   ],
   "source": [
    "print(dataSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동적인 방식의 데이터셋 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일을 하나씩 읽는 것을 가정(운영 환경에서는 이렇게 사용하지 말 것)\n",
    "# 데이터 형을 자동으로 추론하는 것은 비추(예제에서는 위에서 읽은 데이터형을 재활용)\n",
    "streaming = spark\\\n",
    "    .readStream\\\n",
    "    .schema(dataSchema)\\\n",
    "    .option('maxFilesPerTrigger', 2)\\\n",
    "    .json(\"../data/activity-data/\")\n",
    "\n",
    "# 트렌스포메이션\n",
    "activityCounts = streaming.groupBy('gt').count()\n",
    "\n",
    "# 결과를 메모리에 저장하도록 메모리 싱크 설정\n",
    "activityQuery = activityCounts\\\n",
    "    .writeStream\\\n",
    "    .queryName('activity_counts')\\\n",
    "    .format('memory')\\\n",
    "    .outputMode('complete')\\\n",
    "    .start()\n",
    "\n",
    "# 실행\n",
    "# activityQuery.awaitTermination() # 쿼리 종료 시 까지 대기함, background에서 실행됨\n",
    "                                   # 운영 애플리케이션에서는 반드시 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f0472cbe210>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스트림 목록을 확인할 수 있음\n",
    "# UUID가 부여되어 다시 선택할 수 있지만 여기서는 변수에 할당했으므로 변수 사용이 유리함\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|73850|\n",
      "|     stand|68309|\n",
      "|stairsdown|56186|\n",
      "|      walk|79536|\n",
      "|  stairsup|62715|\n",
      "|      null|62690|\n",
      "|      bike|64785|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|172317|\n",
      "|     stand|159385|\n",
      "|stairsdown|131088|\n",
      "|      walk|185585|\n",
      "|  stairsup|146368|\n",
      "|      null|146263|\n",
      "|      bike|151163|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|270779|\n",
      "|     stand|250466|\n",
      "|stairsdown|205978|\n",
      "|      walk|291633|\n",
      "|  stairsup|230032|\n",
      "|      null|229834|\n",
      "|      bike|237541|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "for x in range(3):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activityQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.4 스트림 트랜스포메이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.4.1 선택과 필터링\n",
    "+ 키를 변경하지 않으므로 append 출력 모드를 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "simpleTransfrom = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    "    .where(\"stairs\")\\\n",
    "    .where(\"gt is not null\")\\\n",
    "    .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"simple_transform\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.4.2 집계\n",
    "+ 7장 스트림에 적용 가능한 함수 참조\n",
    "+ 원시 컬럼에 대한 집계 외 이벤트 시간 컬럼 지정, 워터마크, 윈도우 처리를 지원함 (22장 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    "    .drop(\"avg(Arrival_time)\")\\\n",
    "    .drop(\"avg(Creation_time)\")\\\n",
    "    .drop(\"avg(Index)\")\\\n",
    "    .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|       sit|  null|-5.15907918915406...|2.734365086787861...|-1.41574357215394...|\n",
      "|     stand|  null|-3.75961836153627...|4.111774313006433...|3.049861196337197...|\n",
      "|       sit|nexus4|-5.15907918915406...|2.734365086787861...|-1.41574357215394...|\n",
      "|     stand|nexus4|-3.75961836153627...|4.111774313006433...|3.049861196337197...|\n",
      "|      null|  null|-0.00699334693654679|-0.00117077767059...|0.005646005653992315|\n",
      "|      null|  null|0.001510247341501...|-0.00696338577209...|-0.00910733461973...|\n",
      "|      walk|  null|-0.00304135060055069|0.004227462440234608|-6.40891870085255...|\n",
      "|      null|nexus4|-0.00699334693654679|-0.00117077767059...|0.005646005653992315|\n",
      "|      null|nexus4|0.001510247341501...|-0.00696338577209...|-0.00910733461973...|\n",
      "|      bike|  null|0.025043180776785214|-0.01054190537931...|-0.08249728393481527|\n",
      "|  stairsup|  null|-0.02566381095830...|-0.01156850181701786|-0.10024452219597682|\n",
      "|stairsdown|  null| 0.02559440210225853|-0.03847850318554954| 0.12555594831095865|\n",
      "|      bike|nexus4|0.025043180776785214|-0.01054190537931...|-0.08249728393481527|\n",
      "|      walk|nexus4|-0.00304135060055069|0.004227462440234608|-6.40891870085255...|\n",
      "|stairsdown|nexus4| 0.02559440210225853|-0.03847850318554954| 0.12555594831095865|\n",
      "|  stairsup|nexus4|-0.02566381095830...|-0.01156850181701786|-0.10024452219597682|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|       sit|  null|-5.07251451936709...|3.073807798549758...|-1.30352888086243...|\n",
      "|     stand|  null|-3.77056051897803...|3.774624836383798E-4|2.271646020037544...|\n",
      "|       sit|nexus4|-5.07251451936709...|3.073807798549758...|-1.30352888086243...|\n",
      "|     stand|nexus4|-3.77056051897803...|3.774624836383798E-4|2.271646020037544...|\n",
      "|      null|  null|-0.00770515338670...|-9.86285626979075...|0.003953144322972481|\n",
      "|      null|  null|8.186511643770674E-4|-0.00692948546484...|-0.00897082575848...|\n",
      "|      walk|  null|-0.00420626560819...|0.003293984745767...|3.655688175541692E-5|\n",
      "|      null|nexus4|-0.00770515338670...|-9.86285626979075...|0.003953144322972481|\n",
      "|      null|nexus4|8.186511643770674E-4|-0.00692948546484...|-0.00897082575848...|\n",
      "|      bike|  null|0.023019073465984023|-0.00951191658301...|-0.08299550917143556|\n",
      "|  stairsup|  null|-0.02626317293005...|-0.00937862856730...|-0.09842125977710868|\n",
      "|stairsdown|  null|0.025267384130684196|-0.04071558758727276| 0.12624512404546434|\n",
      "|      bike|nexus4|0.023019073465984023|-0.00951191658301...|-0.08299550917143556|\n",
      "|      walk|nexus4|-0.00420626560819...|0.003293984745767...|3.655688175541692E-5|\n",
      "|stairsdown|nexus4|0.025267384130684196|-0.04071558758727276| 0.12624512404546434|\n",
      "|  stairsup|nexus4|-0.02626317293005...|-0.00937862856730...|-0.09842125977710868|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|       sit|  null|-4.99368126296369...|2.512465019316057E-4|-1.78368257768058...|\n",
      "|     stand|  null|-3.53324114394988...|4.068373706090748E-4|2.175775439366989E-4|\n",
      "|       sit|nexus4|-4.99368126296369...|2.512465019316057E-4|-1.78368257768058...|\n",
      "|     stand|nexus4|-3.53324114394988...|4.068373706090748E-4|2.175775439366989E-4|\n",
      "|      null|  null|-0.00758325702758...|-6.60492931776378E-4| 0.00495966581838116|\n",
      "|      null|  null|7.675859038081823E-4|-0.00643620860750435|-0.00896069060137...|\n",
      "|      walk|  null|-0.00398943559989...|0.003045739166713809|-3.53792977582568...|\n",
      "|      null|nexus4|-0.00758325702758...|-6.60492931776378E-4| 0.00495966581838116|\n",
      "|      null|nexus4|7.675859038081823E-4|-0.00643620860750435|-0.00896069060137...|\n",
      "|      bike|  null|0.023272363877139414|-0.00966707396580097|-0.08249194146979888|\n",
      "|  stairsup|  null|-0.02657683302243776|-0.00862140571595...|-0.09868513795745304|\n",
      "|stairsdown|  null|0.024424167847095685|-0.03724819948800649| 0.12512176269372682|\n",
      "|      bike|nexus4|0.023272363877139414|-0.00966707396580097|-0.08249194146979888|\n",
      "|      walk|nexus4|-0.00398943559989...|0.003045739166713809|-3.53792977582568...|\n",
      "|stairsdown|nexus4|0.024424167847095685|-0.03724819948800649| 0.12512176269372682|\n",
      "|  stairsup|nexus4|-0.02657683302243776|-0.00862140571595...|-0.09868513795745304|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(3):\n",
    "    spark.sql(\"SELECT * FROM device_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceModelStats.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.5 입력과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 소스와 싱크\n",
    "+ 스트리밍에서 파일 소스/싱크와 정적 파일 소스를 사용할 때 유일한 차이점은 트리거 시 읽을 파일 수를 결정할 수 있다는 점임(maxFilesPerTrigger 옵션)\n",
    "+ 입력 디텍더리에 원자적으로 추가되어하며, 그렇지 않으면 파일의 일부분만 처리됨\n",
    "+ 외부 디렉터리에 파일을 완전히 기록한 후 입력 디텍터리로 옮겨야 함(아마존 S3에서는 완전히 기록된 객체만 보임)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카프카 소스와 싱크\n",
    "+ 카프카는 분산형 버퍼로 생각할 수 있음\n",
    "+ 순서를 바꿀 수 없는 레코드로 구성되며 레코드의 위치를 오프셋이라고 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.5.2 카프카 소스에서 메시지 읽기\n",
    "+ 아래 옵션 중 하나를 선택\n",
    "    + assign : 토픽뿐만 아니라 읽으려는 파티션까지 세밀하게 지정하는 옵션, ex) {\"topicA\":[0, 1], \"topicB\":[2, 4]}\n",
    "    + subscribe : 토픽 목록턴을 지정해 여러 토픽을 구독\n",
    "    + subscribePattern : 토픽 패턴을 지정해 여러 토픽을 구독\n",
    "+ 카프카 서비스에 접속할 수 있도록 kafka.bootstrap.servers 값을 지정\n",
    "+ 기타\n",
    "    + startingOffsets / endingOffsets\n",
    "        + 쿼리를 시작할 때 읽을 지점 설정\n",
    "        + -2는 earliset, -1는 latest{{\"topicA\" : {\"1\"}:-1}, \"topicB\" : {\"0\"}:-2}}\n",
    "        + 새로운 스트리밍 쿼리가 시작될 때만 적용\n",
    "    + failOnDataLoss\n",
    "        + 데이터 유실(예 : 토픽이 삭제되거나 오프셋이 범위를 벗어났을 때) 쿼리를 중단할 것인지 지정\n",
    "        + 기본값은 true\n",
    "    + maxOffsetsPerTrigger\n",
    "        + 특정 트리거 시점에 읽을 오프셋의 전체 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Read messages from kafka \"\"\"\n",
    "\n",
    "# one topic\n",
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.server\", \"host1:port1\", \"host2:port2\")\\\n",
    "    .option(\"subscribe\", \"topic1\")\\\n",
    "    .load()\n",
    "\n",
    "# multi topics\n",
    "df2 = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.server\", \"host1:port1\", \"host2:port2\")\\\n",
    "    .option(\"subscribe\", \"topic1,topic2\")\\\n",
    "    .load()\n",
    "\n",
    "# topics which has patterns we want\n",
    "df3 = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.server\", \"host1:port1\", \"host2:port2\")\\\n",
    "    .option(\"subscribePattern\", \"topic1.*\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.5.3 카프카 싱크에 메시지 쓰기\n",
    "+ 읽은 쿼리와 매우 비슷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 동일한 역할을 함 \"\"\"\n",
    "\n",
    "df1.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
    "  .writeStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    "  .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n",
    "  .start()\n",
    "\n",
    "df1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
    "  .writeStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    "  .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n",
    "  .option(\"topic\", \"topic1\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach 싱크\n",
    "+ foreachPartitions과 유사하게 각 파티션에서 임의의 연산을 병렬로 수행\n",
    "+ ForeachWriter 인터페이스를 구현해야 함\n",
    "+ 스칼라와 자바만 지원하며, open, process, close 세가지 메서드를 지님\n",
    "    + open \n",
    "    + process : 데이터를 처리하거나 저장하는 용도\n",
    "    + close : 스트림 처리 도중 오류가 발생하면 close 메서도도 호출됨\n",
    "\n",
    "```\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "\n",
    "datasetOfString.write.foreach(new ForeachWriter[String] {\n",
    "  def open(partitionId: Long, version: Long): Boolean = {\n",
    "    // open a database connection\n",
    "  }\n",
    "  def process(record: String) = {\n",
    "    // write string to connection\n",
    "  }\n",
    "  def close(errorOrNull: Throwable): Unit = {\n",
    "    // close the connection\n",
    "  }\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트용 소스와 싱크\n",
    "+ 운영 환경에서는 절대 사용하지 말 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 소켓 소스 \"\"\"\n",
    "socketDF = spark.readStream.format(\"socket\")\\\n",
    "    .option(\"host\", \"localhost\").option(\"port\", 9999).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nc -lk 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f04908bdc10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 콘솔 싱크 \"\"\"\n",
    "activityCounts.writeStream.format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f04908a28d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 메모리 싱크 \"\"\"\n",
    "activityCounts\\\n",
    "    .writeStream.queryName(\"exsample\").format(\"memory\")\\\n",
    "    .outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.5.4 데이터 출력 방법(출력 모드)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### append\n",
    "+ 새로운 로우가 결과 테이블에 추가되면 사용자가 명시한 트리거에 맞춰 싱크로 출력됨\n",
    "+ 이벤트 시간과 워터마크를 사용하면 최종 결과만 싱크로 출력됨(22장)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### complete\n",
    "+ 결과 테이블의 전체 상태를 싱크로 출력\n",
    "+ 데이터가 계속해서 변경될 수 있는 일부 상태 기반 데이터를 다룰 때 유용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update\n",
    "+ 변경된 로우만 싱크로 출력하는 점을 제외하면 complete 모두와 유사\n",
    "+ 쿼리에서 집계 연산을 하지 않는다면 append와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.5.5 데이터 출력 시점(트리거)\n",
    "+ 싱크에 큰 부하가 발생하는 현상을 방지하거나 출력 파일의 크기를 제어하는 용도로 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f04907ff390>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 처리시간 기반 트리거 \"\"\"\n",
    "\n",
    "activityCounts.writeStream.trigger(processingTime='5 seconds')\\\n",
    "    .format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f04907ffe10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 일회성 트리거 \"\"\"\n",
    "\n",
    "# 개발 중 테스트하거나 운영 환경에서 자주 실행되지 않는 잡을 수동으로 실행할 때 유용\n",
    "activityCounts.writeStream.trigger(once=True)\\\n",
    "    .format(\"console\").outputMode(\"complete\").start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
